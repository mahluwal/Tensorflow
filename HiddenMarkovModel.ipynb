{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HiddenMarkovModel.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMO517iEdvq7+S8cXCChC1b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahluwal/Tensorflow/blob/master/HiddenMarkovModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBttMwiyAoNm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0x7HmRUC2fa"
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ============================================================================\n",
        "\"\"\"The HiddenMarkovModel distribution class.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "from tensorflow_probability.python.distributions import categorical\n",
        "from tensorflow_probability.python.distributions import distribution\n",
        "from tensorflow_probability.python.internal import assert_util\n",
        "from tensorflow_probability.python.internal import distribution_util\n",
        "from tensorflow_probability.python.internal import dtype_util\n",
        "from tensorflow_probability.python.internal import prefer_static\n",
        "from tensorflow_probability.python.internal import reparameterization\n",
        "from tensorflow_probability.python.internal import tensor_util\n",
        "from tensorflow_probability.python.internal import tensorshape_util\n",
        "from tensorflow_probability.python.util.seed_stream import SeedStream\n",
        "from tensorflow.python.util import deprecation  # pylint: disable=g-direct-tensorflow-import\n",
        "\n",
        "\n",
        "__all__ = [\n",
        "    'HiddenMarkovModel',\n",
        "]\n",
        "\n",
        "\n",
        "class HiddenMarkovModel(distribution.Distribution):\n",
        "  \"\"\"Hidden Markov model distribution.\n",
        "  The `HiddenMarkovModel` distribution implements a (batch of) hidden\n",
        "  Markov models where the initial states, transition probabilities\n",
        "  and observed states are all given by user-provided distributions.\n",
        "  This model assumes that the transition matrices are fixed over time.\n",
        "  In this model, there is a sequence of integer-valued hidden states:\n",
        "  `z[0], z[1], ..., z[num_steps - 1]` and a sequence of observed states:\n",
        "  `x[0], ..., x[num_steps - 1]`.\n",
        "  The distribution of `z[0]` is given by `initial_distribution`.\n",
        "  The conditional probability of `z[i  +  1]` given `z[i]` is described by\n",
        "  the batch of distributions in `transition_distribution`.\n",
        "  For a batch of hidden Markov models, the coordinates before the rightmost one\n",
        "  of the `transition_distribution` batch correspond to indices into the hidden\n",
        "  Markov model batch. The rightmost coordinate of the batch is used to select\n",
        "  which distribution `z[i + 1]` is drawn from.  The distributions corresponding\n",
        "  to the probability of `z[i + 1]` conditional on `z[i] == k` is given by the\n",
        "  elements of the batch whose rightmost coordinate is `k`.\n",
        "  Similarly, the conditional distribution of `z[i]` given `x[i]` is given by\n",
        "  the batch of `observation_distribution`.\n",
        "  When the rightmost coordinate of `observation_distribution` is `k` it\n",
        "  gives the conditional probabilities of `x[i]` given `z[i] == k`.\n",
        "  The probability distribution associated with the `HiddenMarkovModel`\n",
        "  distribution is the marginal distribution of `x[0],...,x[num_steps - 1]`.\n",
        "  #### Examples\n",
        "  ```python\n",
        "  tfd = tfp.distributions\n",
        "  # A simple weather model.\n",
        "  # Represent a cold day with 0 and a hot day with 1.\n",
        "  # Suppose the first day of a sequence has a 0.8 chance of being cold.\n",
        "  # We can model this using the categorical distribution:\n",
        "  initial_distribution = tfd.Categorical(probs=[0.8, 0.2])\n",
        "  # Suppose a cold day has a 30% chance of being followed by a hot day\n",
        "  # and a hot day has a 20% chance of being followed by a cold day.\n",
        "  # We can model this as:\n",
        "  transition_distribution = tfd.Categorical(probs=[[0.7, 0.3],\n",
        "                                                   [0.2, 0.8]])\n",
        "  # Suppose additionally that on each day the temperature is\n",
        "  # normally distributed with mean and standard deviation 0 and 5 on\n",
        "  # a cold day and mean and standard deviation 15 and 10 on a hot day.\n",
        "  # We can model this with:\n",
        "  observation_distribution = tfd.Normal(loc=[0., 15.], scale=[5., 10.])\n",
        "  # We can combine these distributions into a single week long\n",
        "  # hidden Markov model with:\n",
        "  model = tfd.HiddenMarkovModel(\n",
        "      initial_distribution=initial_distribution,\n",
        "      transition_distribution=transition_distribution,\n",
        "      observation_distribution=observation_distribution,\n",
        "      num_steps=7)\n",
        "  # The expected temperatures for each day are given by:\n",
        "  model.mean()  # shape [7], elements approach 9.0\n",
        "  # The log pdf of a week of temperature 0 is:\n",
        "  model.log_prob(tf.zeros(shape=[7]))\n",
        "  ```\n",
        "  #### References\n",
        "  [1] https://en.wikipedia.org/wiki/Hidden_Markov_model\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               initial_distribution,\n",
        "               transition_distribution,\n",
        "               observation_distribution,\n",
        "               num_steps,\n",
        "               validate_args=False,\n",
        "               allow_nan_stats=True,\n",
        "               name='HiddenMarkovModel'):\n",
        "    \"\"\"Initialize hidden Markov model.\n",
        "    Args:\n",
        "      initial_distribution: A `Categorical`-like instance.\n",
        "        Determines probability of first hidden state in Markov chain.\n",
        "        The number of categories must match the number of categories of\n",
        "        `transition_distribution` as well as both the rightmost batch\n",
        "        dimension of `transition_distribution` and the rightmost batch\n",
        "        dimension of `observation_distribution`.\n",
        "      transition_distribution: A `Categorical`-like instance.\n",
        "        The rightmost batch dimension indexes the probability distribution\n",
        "        of each hidden state conditioned on the previous hidden state.\n",
        "      observation_distribution: A `tfp.distributions.Distribution`-like\n",
        "        instance.  The rightmost batch dimension indexes the distribution\n",
        "        of each observation conditioned on the corresponding hidden state.\n",
        "      num_steps: The number of steps taken in Markov chain. An integer valued\n",
        "        tensor. The number of transitions is `num_steps - 1`.\n",
        "      validate_args: Python `bool`, default `False`. When `True` distribution\n",
        "        parameters are checked for validity despite possibly degrading runtime\n",
        "        performance. When `False` invalid inputs may silently render incorrect\n",
        "        outputs.\n",
        "        Default value: `False`.\n",
        "      allow_nan_stats: Python `bool`, default `True`. When `True`, statistics\n",
        "        (e.g., mean, mode, variance) use the value \"`NaN`\" to indicate the\n",
        "        result is undefined. When `False`, an exception is raised if one or\n",
        "        more of the statistic's batch members are undefined.\n",
        "        Default value: `True`.\n",
        "      name: Python `str` name prefixed to Ops created by this class.\n",
        "        Default value: \"HiddenMarkovModel\".\n",
        "    Raises:\n",
        "      ValueError: if `num_steps` is not at least 1.\n",
        "      ValueError: if `initial_distribution` does not have scalar `event_shape`.\n",
        "      ValueError: if `transition_distribution` does not have scalar\n",
        "        `event_shape.`\n",
        "      ValueError: if `transition_distribution` and `observation_distribution`\n",
        "        are fully defined but don't have matching rightmost dimension.\n",
        "    \"\"\"\n",
        "\n",
        "    parameters = dict(locals())\n",
        "\n",
        "    # pylint: disable=protected-access\n",
        "    with tf.name_scope(name) as name:\n",
        "\n",
        "      self._num_steps = tensor_util.convert_nonref_to_tensor(num_steps)\n",
        "      self._initial_distribution = initial_distribution\n",
        "      self._observation_distribution = observation_distribution\n",
        "      self._transition_distribution = transition_distribution\n",
        "\n",
        "      num_steps_ = tf.get_static_value(num_steps)\n",
        "      if num_steps_ is not None:\n",
        "        if np.ndim(num_steps_) != 0:\n",
        "          raise ValueError(\n",
        "              '`num_steps` must be a scalar but it has rank {}'.format(\n",
        "                  np.ndim(num_steps_)))\n",
        "        else:\n",
        "          self._static_event_shape = tf.TensorShape(\n",
        "              [num_steps_]).concatenate(\n",
        "                  self._observation_distribution.event_shape)\n",
        "      else:\n",
        "        self._static_event_shape = tf.TensorShape(\n",
        "            [None]).concatenate(\n",
        "                self._observation_distribution.event_shape)\n",
        "\n",
        "      self._static_batch_shape = tf.broadcast_static_shape(\n",
        "          self._initial_distribution.batch_shape,\n",
        "          tf.broadcast_static_shape(\n",
        "              self._transition_distribution.batch_shape[:-1],\n",
        "              self._observation_distribution.batch_shape[:-1]))\n",
        "\n",
        "      # pylint: disable=protected-access\n",
        "      super(HiddenMarkovModel, self).__init__(\n",
        "          dtype=self._observation_distribution.dtype,\n",
        "          reparameterization_type=reparameterization.NOT_REPARAMETERIZED,\n",
        "          validate_args=validate_args,\n",
        "          allow_nan_stats=allow_nan_stats,\n",
        "          parameters=parameters,\n",
        "          name=name)\n",
        "      # pylint: enable=protected-access\n",
        "\n",
        "      self._parameters = parameters\n",
        "\n",
        "  def _batch_shape_tensor(self):\n",
        "    return tf.broadcast_dynamic_shape(\n",
        "        self._initial_distribution.batch_shape_tensor(),\n",
        "        tf.broadcast_dynamic_shape(\n",
        "            self._transition_distribution.batch_shape_tensor()[:-1],\n",
        "            self._observation_distribution.batch_shape_tensor()[:-1]))\n",
        "\n",
        "  def _batch_shape(self):\n",
        "    return self._static_batch_shape\n",
        "\n",
        "  def _event_shape_tensor(self):\n",
        "    return tf.concat([[self._num_steps],\n",
        "                      self.observation_distribution.event_shape_tensor()],\n",
        "                     axis=0)\n",
        "\n",
        "  def _event_shape(self):\n",
        "    return self._static_event_shape\n",
        "\n",
        "  @property\n",
        "  def initial_distribution(self):\n",
        "    return self._initial_distribution\n",
        "\n",
        "  @property\n",
        "  def transition_distribution(self):\n",
        "    return self._transition_distribution\n",
        "\n",
        "  @property\n",
        "  def observation_distribution(self):\n",
        "    return self._observation_distribution\n",
        "\n",
        "  @property\n",
        "  def num_steps(self):\n",
        "    return self._num_steps\n",
        "\n",
        "  @property\n",
        "  @deprecation.deprecated(\n",
        "      '2020-02-08',\n",
        "      'Use `num_states_static` or `num_states_tensor` instead.')\n",
        "  def num_states(self):\n",
        "    return self.num_states_tensor\n",
        "\n",
        "  @property\n",
        "  def num_states_static(self):\n",
        "    \"\"\"The number of hidden states in the hidden Markov model.\n",
        "    Returns:\n",
        "      A value of integer type if the number of states can be computed\n",
        "      statically and `None` otherwise.\n",
        "    \"\"\"\n",
        "\n",
        "    return tf.get_static_value(self.transition_distribution.batch_shape[-1])\n",
        "\n",
        "  def num_states_tensor(self):\n",
        "    \"\"\"The number of hidden states in the hidden Markov model.\"\"\"\n",
        "\n",
        "    return self.transition_distribution.batch_shape_tensor()[-1]\n",
        "\n",
        "  def _sample_n(self, n, seed=None):\n",
        "    strm = SeedStream(seed, salt='HiddenMarkovModel')\n",
        "\n",
        "    transition_batch_shape = self.transition_distribution.batch_shape_tensor()\n",
        "    num_states = transition_batch_shape[-1]\n",
        "\n",
        "    batch_shape = self.batch_shape_tensor()\n",
        "    batch_size = tf.reduce_prod(batch_shape)\n",
        "\n",
        "    # The batch sizes of the underlying initial distributions and\n",
        "    # transition distributions might not match the batch size of\n",
        "    # the HMM distribution.\n",
        "    # As a result we need to ask for more samples from the\n",
        "    # underlying distributions and then reshape the results into\n",
        "    # the correct batch size for the HMM.\n",
        "    init_repeat = (\n",
        "        tf.reduce_prod(batch_shape) //\n",
        "        tf.reduce_prod(self._initial_distribution.batch_shape_tensor()))\n",
        "    init_state = self._initial_distribution.sample(n * init_repeat,\n",
        "                                                   seed=strm())\n",
        "    init_state = tf.reshape(init_state, [n, batch_size])\n",
        "    # init_state :: n batch_size\n",
        "\n",
        "    transition_repeat = (\n",
        "        tf.reduce_prod(batch_shape) // tf.reduce_prod(\n",
        "            transition_batch_shape[:-1]))\n",
        "\n",
        "    init_shape = init_state.shape\n",
        "\n",
        "    def generate_step(state, _):\n",
        "      \"\"\"Take a single step in Markov chain.\"\"\"\n",
        "\n",
        "      gen = self._transition_distribution.sample(n * transition_repeat,\n",
        "                                                 seed=strm())\n",
        "      # gen :: (n * transition_repeat) transition_batch\n",
        "\n",
        "      new_states = tf.reshape(gen,\n",
        "                              [n, batch_size, num_states])\n",
        "\n",
        "      # new_states :: n batch_size num_states\n",
        "\n",
        "      old_states_one_hot = tf.one_hot(state, num_states, dtype=tf.int32)\n",
        "\n",
        "      # old_states :: n batch_size num_states\n",
        "\n",
        "      result = tf.reduce_sum(old_states_one_hot * new_states, axis=-1)\n",
        "      # We know that `generate_step` must preserve the shape of the\n",
        "      # tensor of states of each state. This is because\n",
        "      # the transition matrix must be square. But TensorFlow might\n",
        "      # not know this so we explicitly tell it that the result has the\n",
        "      # same shape.\n",
        "      tensorshape_util.set_shape(result, init_shape)\n",
        "      return result\n",
        "\n",
        "    def _scan_multiple_steps():\n",
        "      \"\"\"Take multiple steps with tf.scan.\"\"\"\n",
        "      dummy_index = tf.zeros(self._num_steps - 1, dtype=tf.float32)\n",
        "      if seed is not None:\n",
        "        # Force parallel_iterations to 1 to ensure reproducibility\n",
        "        # b/139210489\n",
        "        hidden_states = tf.scan(generate_step, dummy_index,\n",
        "                                initializer=init_state,\n",
        "                                parallel_iterations=1)\n",
        "      else:\n",
        "        # Invoke default parallel_iterations behavior\n",
        "        hidden_states = tf.scan(generate_step, dummy_index,\n",
        "                                initializer=init_state)\n",
        "\n",
        "      # TODO(b/115618503): add/use prepend_initializer to tf.scan\n",
        "      return tf.concat([[init_state],\n",
        "                        hidden_states], axis=0)\n",
        "\n",
        "    hidden_states = prefer_static.cond(\n",
        "        self._num_steps > 1,\n",
        "        _scan_multiple_steps,\n",
        "        lambda: init_state[tf.newaxis, ...])\n",
        "\n",
        "    hidden_one_hot = tf.one_hot(hidden_states, num_states,\n",
        "                                dtype=self._observation_distribution.dtype)\n",
        "    # hidden_one_hot :: num_steps n batch_size num_states\n",
        "\n",
        "    # The observation distribution batch size might not match\n",
        "    # the required batch size so as with the initial and\n",
        "    # transition distributions we generate more samples and\n",
        "    # reshape.\n",
        "    observation_repeat = (\n",
        "        batch_size // tf.reduce_prod(\n",
        "            self._observation_distribution.batch_shape_tensor()[:-1]))\n",
        "\n",
        "    possible_observations = self._observation_distribution.sample(\n",
        "        [self._num_steps, observation_repeat * n], seed=strm())\n",
        "\n",
        "    inner_shape = self._observation_distribution.event_shape_tensor()\n",
        "\n",
        "    # possible_observations :: num_steps (observation_repeat * n)\n",
        "    #                          observation_batch[:-1] num_states inner_shape\n",
        "\n",
        "    possible_observations = tf.reshape(\n",
        "        possible_observations,\n",
        "        tf.concat([[self._num_steps, n],\n",
        "                   batch_shape,\n",
        "                   [num_states],\n",
        "                   inner_shape], axis=0))\n",
        "\n",
        "    # possible_observations :: steps n batch_size num_states inner_shape\n",
        "\n",
        "    hidden_one_hot = tf.reshape(hidden_one_hot,\n",
        "                                tf.concat([[self._num_steps, n],\n",
        "                                           batch_shape,\n",
        "                                           [num_states],\n",
        "                                           tf.ones_like(inner_shape)],\n",
        "                                          axis=0))\n",
        "\n",
        "    # hidden_one_hot :: steps n batch_size num_states \"inner_shape\"\n",
        "\n",
        "    observations = tf.reduce_sum(\n",
        "        hidden_one_hot * possible_observations,\n",
        "        axis=-1 - tf.size(inner_shape))\n",
        "\n",
        "    # observations :: steps n batch_size inner_shape\n",
        "\n",
        "    observations = distribution_util.move_dimension(observations, 0,\n",
        "                                                    1 + tf.size(batch_shape))\n",
        "\n",
        "    # returned :: n batch_shape steps inner_shape\n",
        "\n",
        "    return observations\n",
        "\n",
        "  def _log_prob(self, value):\n",
        "    # The argument `value` is a tensor of sequences of observations.\n",
        "    # `observation_batch_shape` is the shape of that tensor with the\n",
        "    # sequence part removed.\n",
        "    # `observation_batch_shape` is then broadcast to the full batch shape\n",
        "    # to give the `batch_shape` that defines the shape of the result.\n",
        "\n",
        "    observation_tensor_shape = tf.shape(value)\n",
        "    observation_distribution = self.observation_distribution\n",
        "    underlying_event_rank = tf.size(\n",
        "        observation_distribution.event_shape_tensor())\n",
        "    observation_batch_shape = observation_tensor_shape[\n",
        "        :-1 - underlying_event_rank]\n",
        "    # value :: observation_batch_shape num_steps observation_event_shape\n",
        "    batch_shape = tf.broadcast_dynamic_shape(observation_batch_shape,\n",
        "                                             self.batch_shape_tensor())\n",
        "    num_states = self.transition_distribution.batch_shape_tensor()[-1]\n",
        "    log_init = _extract_log_probs(num_states,\n",
        "                                  self.initial_distribution)\n",
        "    # log_init :: batch_shape num_states\n",
        "    log_init = tf.broadcast_to(log_init,\n",
        "                               tf.concat([batch_shape,\n",
        "                                          [num_states]], axis=0))\n",
        "    log_transition = _extract_log_probs(num_states,\n",
        "                                        self.transition_distribution)\n",
        "\n",
        "    # `observation_event_shape` is the shape of each sequence of observations\n",
        "    # emitted by the model.\n",
        "    observation_event_shape = observation_tensor_shape[\n",
        "        -1 - underlying_event_rank:]\n",
        "    working_obs = tf.broadcast_to(value,\n",
        "                                  tf.concat([batch_shape,\n",
        "                                             observation_event_shape],\n",
        "                                            axis=0))\n",
        "    # working_obs :: batch_shape observation_event_shape\n",
        "    r = underlying_event_rank\n",
        "\n",
        "    # Move index into sequence of observations to front so we can apply\n",
        "    # tf.foldl\n",
        "    working_obs = distribution_util.move_dimension(working_obs, -1 - r,\n",
        "                                                   0)[..., tf.newaxis]\n",
        "    # working_obs :: num_steps batch_shape underlying_event_shape\n",
        "    observation_probs = (\n",
        "        observation_distribution.log_prob(working_obs))\n",
        "\n",
        "    def forward_step(log_prev_step, log_prob_observation):\n",
        "      return _log_vector_matrix(log_prev_step,\n",
        "                                log_transition) + log_prob_observation\n",
        "\n",
        "    fwd_prob = tf.foldl(forward_step, observation_probs, initializer=log_init)\n",
        "    # fwd_prob :: batch_shape num_states\n",
        "\n",
        "    log_prob = tf.reduce_logsumexp(fwd_prob, axis=-1)\n",
        "    # log_prob :: batch_shape\n",
        "\n",
        "    return log_prob\n",
        "\n",
        "  def _marginal_hidden_probs(self):\n",
        "    \"\"\"Compute marginal pdf for each individual observable.\"\"\"\n",
        "\n",
        "    num_states = self.transition_distribution.batch_shape_tensor()[-1]\n",
        "    log_init = _extract_log_probs(num_states,\n",
        "                                  self.initial_distribution)\n",
        "    initial_log_probs = tf.broadcast_to(log_init,\n",
        "                                        tf.concat([self.batch_shape_tensor(),\n",
        "                                                   [num_states]],\n",
        "                                                  axis=0))\n",
        "\n",
        "    # initial_log_probs :: batch_shape num_states\n",
        "\n",
        "    no_transition_result = initial_log_probs[tf.newaxis, ...]\n",
        "\n",
        "    def _scan_multiple_steps():\n",
        "      \"\"\"Perform `scan` operation when `num_steps` > 1.\"\"\"\n",
        "\n",
        "      transition_log_probs = _extract_log_probs(num_states,\n",
        "                                                self.transition_distribution)\n",
        "\n",
        "      def forward_step(log_probs, _):\n",
        "        result = _log_vector_matrix(log_probs, transition_log_probs)\n",
        "        # We know that `forward_step` must preserve the shape of the\n",
        "        # tensor of probabilities of each state. This is because\n",
        "        # the transition matrix must be square. But TensorFlow might\n",
        "        # not know this so we explicitly tell it that the result has the\n",
        "        # same shape.\n",
        "        result.set_shape(log_probs.shape)\n",
        "        return result\n",
        "\n",
        "      dummy_index = tf.zeros(self._num_steps - 1, dtype=tf.float32)\n",
        "\n",
        "      forward_log_probs = tf.scan(forward_step, dummy_index,\n",
        "                                  initializer=initial_log_probs,\n",
        "                                  name='forward_log_probs')\n",
        "\n",
        "      result = tf.concat([[initial_log_probs], forward_log_probs],\n",
        "                         axis=0)\n",
        "      return result\n",
        "\n",
        "    forward_log_probs = prefer_static.cond(\n",
        "        self._num_steps > 1,\n",
        "        _scan_multiple_steps,\n",
        "        lambda: no_transition_result)\n",
        "\n",
        "    return tf.exp(forward_log_probs)\n",
        "\n",
        "  def _mean(self):\n",
        "    observation_distribution = self.observation_distribution\n",
        "    batch_shape = self.batch_shape_tensor()\n",
        "    num_states = self.transition_distribution.batch_shape_tensor()[-1]\n",
        "    probs = self._marginal_hidden_probs()\n",
        "    # probs :: num_steps batch_shape num_states\n",
        "    means = observation_distribution.mean()\n",
        "    # means :: observation_batch_shape[:-1] num_states\n",
        "    #          observation_event_shape\n",
        "    means_shape = tf.concat(\n",
        "        [batch_shape,\n",
        "         [num_states],\n",
        "         observation_distribution.event_shape_tensor()],\n",
        "        axis=0)\n",
        "    means = tf.broadcast_to(means, means_shape)\n",
        "    # means :: batch_shape num_states observation_event_shape\n",
        "\n",
        "    observation_event_shape = (\n",
        "        observation_distribution.event_shape_tensor())\n",
        "    batch_size = tf.reduce_prod(batch_shape)\n",
        "    flat_probs_shape = [self._num_steps, batch_size, num_states]\n",
        "    flat_means_shape = [\n",
        "        batch_size, num_states,\n",
        "        tf.reduce_prod(observation_event_shape)\n",
        "    ]\n",
        "\n",
        "    flat_probs = tf.reshape(probs, flat_probs_shape)\n",
        "    # flat_probs :: num_steps batch_size num_states\n",
        "    flat_means = tf.reshape(means, flat_means_shape)\n",
        "    # flat_means :: batch_size num_states observation_event_size\n",
        "    flat_mean = tf.einsum('ijk,jkl->jil', flat_probs, flat_means)\n",
        "    # flat_mean :: batch_size num_steps observation_event_size\n",
        "    unflat_mean_shape = tf.concat(\n",
        "        [batch_shape,\n",
        "         [self._num_steps],\n",
        "         observation_event_shape],\n",
        "        axis=0)\n",
        "    # returns :: batch_shape num_steps observation_event_shape\n",
        "    return tf.reshape(flat_mean, unflat_mean_shape)\n",
        "\n",
        "  def _variance(self):\n",
        "    num_states = self.transition_distribution.batch_shape_tensor()[-1]\n",
        "    batch_shape = self.batch_shape_tensor()\n",
        "    probs = self._marginal_hidden_probs()\n",
        "    # probs :: num_steps batch_shape num_states\n",
        "    observation_distribution = self.observation_distribution\n",
        "    means = observation_distribution.mean()\n",
        "    # means :: observation_batch_shape[:-1] num_states\n",
        "    #          observation_event_shape\n",
        "    means_shape = tf.concat(\n",
        "        [batch_shape,\n",
        "         [num_states],\n",
        "         observation_distribution.event_shape_tensor()],\n",
        "        axis=0)\n",
        "    means = tf.broadcast_to(means, means_shape)\n",
        "    # means :: batch_shape num_states observation_event_shape\n",
        "\n",
        "    observation_event_shape = (\n",
        "        observation_distribution.event_shape_tensor())\n",
        "    batch_size = tf.reduce_prod(batch_shape)\n",
        "    flat_probs_shape = [self._num_steps, batch_size, num_states]\n",
        "    flat_means_shape = [\n",
        "        batch_size, 1, num_states,\n",
        "        tf.reduce_prod(observation_event_shape)\n",
        "    ]\n",
        "\n",
        "    flat_probs = tf.reshape(probs, flat_probs_shape)\n",
        "    # flat_probs :: num_steps batch_size num_states\n",
        "    flat_means = tf.reshape(means, flat_means_shape)\n",
        "    # flat_means :: batch_size 1 num_states observation_event_size\n",
        "    flat_mean = tf.einsum('ijk,jmkl->jiml', flat_probs, flat_means)\n",
        "    # flat_mean :: batch_size num_steps 1 observation_event_size\n",
        "\n",
        "    variances = observation_distribution.variance()\n",
        "    variances = tf.broadcast_to(variances, means_shape)\n",
        "    # variances :: batch_shape num_states observation_event_shape\n",
        "    flat_variances = tf.reshape(variances, flat_means_shape)\n",
        "    # flat_variances :: batch_size 1 num_states observation_event_size\n",
        "\n",
        "    # For a mixture of n distributions with mixture probabilities\n",
        "    # p[i], and where the individual distributions have means and\n",
        "    # variances given by mean[i] and var[i], the variance of\n",
        "    # the mixture is given by:\n",
        "    #\n",
        "    # var = sum i=1..n p[i] * ((mean[i] - mean)**2 + var[i]**2)\n",
        "\n",
        "    flat_variance = tf.einsum('ijk,jikl->jil',\n",
        "                              flat_probs,\n",
        "                              (flat_means - flat_mean)**2 + flat_variances)\n",
        "    # flat_variance :: batch_size num_steps observation_event_size\n",
        "\n",
        "    unflat_mean_shape = tf.concat(\n",
        "        [batch_shape,\n",
        "         [self._num_steps],\n",
        "         observation_event_shape],\n",
        "        axis=0)\n",
        "\n",
        "    # returns :: batch_shape num_steps observation_event_shape\n",
        "    return tf.reshape(flat_variance, unflat_mean_shape)\n",
        "\n",
        "  def _observation_mask_shape_preconditions(self,\n",
        "                                            observation_tensor_shape,\n",
        "                                            mask_tensor_shape,\n",
        "                                            underlying_event_rank):\n",
        "    shape_condition = [assert_util.assert_equal(\n",
        "        observation_tensor_shape[-1 - underlying_event_rank],\n",
        "        self._num_steps,\n",
        "        message='The tensor `observations` must consist of sequences'\n",
        "                'of observations from `HiddenMarkovModel` of length'\n",
        "                '`num_steps`.')]\n",
        "    if mask_tensor_shape is not None:\n",
        "      shape_condition.append(assert_util.assert_equal(\n",
        "          mask_tensor_shape[-1],\n",
        "          self._num_steps,\n",
        "          message='The tensor `mask` must consist of sequences'\n",
        "                  'of length `num_steps`.'))\n",
        "    return tf.control_dependencies(shape_condition)\n",
        "\n",
        "  def _observation_log_probs(self, observations, mask):\n",
        "    \"\"\"Compute and shape tensor of log probs associated with observations..\"\"\"\n",
        "\n",
        "    # Let E be the underlying event shape\n",
        "    #     M the number of steps in the HMM\n",
        "    #     N the number of states of the HMM\n",
        "    #\n",
        "    # Then the incoming observations have shape\n",
        "    #\n",
        "    # observations : batch_o [M] E\n",
        "    #\n",
        "    # and the mask (if present) has shape\n",
        "    #\n",
        "    # mask : batch_m [M]\n",
        "    #\n",
        "    # Let this HMM distribution have batch shape batch_d\n",
        "    # We need to broadcast all three of these batch shapes together\n",
        "    # into the shape batch.\n",
        "    #\n",
        "    # We need to move the step dimension to the first dimension to make\n",
        "    # them suitable for folding or scanning over.\n",
        "    #\n",
        "    # When we call `log_prob` for our observations we need to\n",
        "    # do this for each state the observation could correspond to.\n",
        "    # We do this by expanding the dimensions by 1 so we end up with:\n",
        "    #\n",
        "    # observations : [M] batch [1] [E]\n",
        "    #\n",
        "    # After calling `log_prob` we get\n",
        "    #\n",
        "    # observation_log_probs : [M] batch [N]\n",
        "    #\n",
        "    # We wish to use `mask` to select from this so we also\n",
        "    # reshape and broadcast it up to shape\n",
        "    #\n",
        "    # mask : [M] batch [N]\n",
        "\n",
        "    observation_distribution = self.observation_distribution\n",
        "    underlying_event_rank = tf.size(\n",
        "        observation_distribution.event_shape_tensor())\n",
        "    observation_tensor_shape = tf.shape(observations)\n",
        "    observation_batch_shape = observation_tensor_shape[\n",
        "        :-1 - underlying_event_rank]\n",
        "    observation_event_shape = observation_tensor_shape[\n",
        "        -1 - underlying_event_rank:]\n",
        "\n",
        "    if mask is not None:\n",
        "      mask_tensor_shape = tf.shape(mask)\n",
        "      mask_batch_shape = mask_tensor_shape[:-1]\n",
        "\n",
        "    batch_shape = tf.broadcast_dynamic_shape(observation_batch_shape,\n",
        "                                             self.batch_shape_tensor())\n",
        "\n",
        "    if mask is not None:\n",
        "      batch_shape = tf.broadcast_dynamic_shape(batch_shape,\n",
        "                                               mask_batch_shape)\n",
        "    observations = tf.broadcast_to(observations,\n",
        "                                   tf.concat([batch_shape,\n",
        "                                              observation_event_shape],\n",
        "                                             axis=0))\n",
        "    observation_rank = tf.rank(observations)\n",
        "    observations = distribution_util.move_dimension(\n",
        "        observations, observation_rank - underlying_event_rank - 1, 0)\n",
        "    observations = tf.expand_dims(\n",
        "        observations,\n",
        "        observation_rank - underlying_event_rank)\n",
        "    observation_log_probs = observation_distribution.log_prob(\n",
        "        observations)\n",
        "\n",
        "    if mask is not None:\n",
        "      mask = tf.broadcast_to(mask,\n",
        "                             tf.concat([batch_shape, [self._num_steps]],\n",
        "                                       axis=0))\n",
        "      mask = distribution_util.move_dimension(mask, -1, 0)\n",
        "      observation_log_probs = tf.where(mask[..., tf.newaxis],\n",
        "                                       tf.zeros_like(observation_log_probs),\n",
        "                                       observation_log_probs)\n",
        "\n",
        "    return observation_log_probs\n",
        "\n",
        "  def posterior_marginals(self, observations, mask=None,\n",
        "                          name='posterior_marginals'):\n",
        "    \"\"\"Compute marginal posterior distribution for each state.\n",
        "    This function computes, for each time step, the marginal\n",
        "    conditional probability that the hidden Markov model was in\n",
        "    each possible state given the observations that were made\n",
        "    at each time step.\n",
        "    So if the hidden states are `z[0],...,z[num_steps - 1]` and\n",
        "    the observations are `x[0], ..., x[num_steps - 1]`, then\n",
        "    this function computes `P(z[i] | x[0], ..., x[num_steps - 1])`\n",
        "    for all `i` from `0` to `num_steps - 1`.\n",
        "    This operation is sometimes called smoothing. It uses a form\n",
        "    of the forward-backward algorithm.\n",
        "    Note: the behavior of this function is undefined if the\n",
        "    `observations` argument represents impossible observations\n",
        "    from the model.\n",
        "    Args:\n",
        "      observations: A tensor representing a batch of observations\n",
        "        made on the hidden Markov model.  The rightmost dimension of this tensor\n",
        "        gives the steps in a sequence of observations from a single sample from\n",
        "        the hidden Markov model. The size of this dimension should match the\n",
        "        `num_steps` parameter of the hidden Markov model object. The other\n",
        "        dimensions are the dimensions of the batch and these are broadcast with\n",
        "        the hidden Markov model's parameters.\n",
        "      mask: optional bool-type `tensor` with rightmost dimension matching\n",
        "        `num_steps` indicating which observations the result of this\n",
        "        function should be conditioned on. When the mask has value\n",
        "        `True` the corresponding observations aren't used.\n",
        "        if `mask` is `None` then all of the observations are used.\n",
        "        the `mask` dimensions left of the last are broadcast with the\n",
        "        hmm batch as well as with the observations.\n",
        "      name: Python `str` name prefixed to Ops created by this class.\n",
        "        Default value: \"HiddenMarkovModel\".\n",
        "    Returns:\n",
        "      posterior_marginal: A `Categorical` distribution object representing the\n",
        "        marginal probability of the hidden Markov model being in each state at\n",
        "        each step. The rightmost dimension of the `Categorical` distributions\n",
        "        batch will equal the `num_steps` parameter providing one marginal\n",
        "        distribution for each step. The other dimensions are the dimensions\n",
        "        corresponding to the batch of observations.\n",
        "    Raises:\n",
        "      ValueError: if rightmost dimension of `observations` does not\n",
        "      have size `num_steps`.\n",
        "    \"\"\"\n",
        "\n",
        "    with self._name_and_control_scope(name):\n",
        "      observation_tensor_shape = tf.shape(observations)\n",
        "      observation_distribution = self.observation_distribution\n",
        "      underlying_event_rank = tf.size(\n",
        "          observation_distribution.event_shape_tensor())\n",
        "      mask_tensor_shape = tf.shape(mask) if mask is not None else None\n",
        "      num_states = self.transition_distribution.batch_shape_tensor()[-1]\n",
        "\n",
        "      with self._observation_mask_shape_preconditions(\n",
        "          observation_tensor_shape, mask_tensor_shape, underlying_event_rank):\n",
        "        observation_log_probs = self._observation_log_probs(\n",
        "            observations, mask)\n",
        "        log_init = _extract_log_probs(num_states,\n",
        "                                      self.initial_distribution)\n",
        "        log_prob = log_init + observation_log_probs[0]\n",
        "        log_transition = _extract_log_probs(num_states,\n",
        "                                            self.transition_distribution)\n",
        "        log_adjoint_prob = tf.zeros_like(log_prob)\n",
        "\n",
        "        def _scan_multiple_steps_forwards():\n",
        "          def forward_step(log_previous_step, log_prob_observation):\n",
        "            return _log_vector_matrix(log_previous_step,\n",
        "                                      log_transition) + log_prob_observation\n",
        "\n",
        "          forward_log_probs = tf.scan(forward_step, observation_log_probs[1:],\n",
        "                                      initializer=log_prob,\n",
        "                                      name='forward_log_probs')\n",
        "          return tf.concat([[log_prob], forward_log_probs], axis=0)\n",
        "\n",
        "        forward_log_probs = prefer_static.cond(\n",
        "            self._num_steps > 1,\n",
        "            _scan_multiple_steps_forwards,\n",
        "            lambda: tf.convert_to_tensor([log_prob]))\n",
        "\n",
        "        total_log_prob = tf.reduce_logsumexp(forward_log_probs[-1], axis=-1)\n",
        "\n",
        "        def _scan_multiple_steps_backwards():\n",
        "          \"\"\"Perform `scan` operation when `num_steps` > 1.\"\"\"\n",
        "\n",
        "          def backward_step(log_previous_step, log_prob_observation):\n",
        "            return _log_matrix_vector(\n",
        "                log_transition,\n",
        "                log_prob_observation + log_previous_step)\n",
        "\n",
        "          backward_log_adjoint_probs = tf.scan(\n",
        "              backward_step,\n",
        "              observation_log_probs[1:],\n",
        "              initializer=log_adjoint_prob,\n",
        "              reverse=True,\n",
        "              name='backward_log_adjoint_probs')\n",
        "\n",
        "          return tf.concat([backward_log_adjoint_probs,\n",
        "                            [log_adjoint_prob]], axis=0)\n",
        "\n",
        "        backward_log_adjoint_probs = prefer_static.cond(\n",
        "            self._num_steps > 1,\n",
        "            _scan_multiple_steps_backwards,\n",
        "            lambda: tf.convert_to_tensor([log_adjoint_prob]))\n",
        "\n",
        "        log_likelihoods = forward_log_probs + backward_log_adjoint_probs\n",
        "\n",
        "        marginal_log_probs = distribution_util.move_dimension(\n",
        "            log_likelihoods - total_log_prob[..., tf.newaxis], 0, -2)\n",
        "\n",
        "        return categorical.Categorical(logits=marginal_log_probs)\n",
        "\n",
        "  def posterior_mode(self, observations, mask=None, name='posterior_mode'):\n",
        "    \"\"\"Compute maximum likelihood sequence of hidden states.\n",
        "    When this function is provided with a sequence of observations\n",
        "    `x[0], ..., x[num_steps - 1]`, it returns the sequence of hidden\n",
        "    states `z[0], ..., z[num_steps - 1]`, drawn from the underlying\n",
        "    Markov chain, that is most likely to yield those observations.\n",
        "    It uses the [Viterbi algorithm](\n",
        "    https://en.wikipedia.org/wiki/Viterbi_algorithm).\n",
        "    Note: the behavior of this function is undefined if the\n",
        "    `observations` argument represents impossible observations\n",
        "    from the model.\n",
        "    Note: if there isn't a unique most likely sequence then one\n",
        "    of the equally most likely sequences is chosen.\n",
        "    Args:\n",
        "      observations: A tensor representing a batch of observations made on the\n",
        "        hidden Markov model.  The rightmost dimensions of this tensor correspond\n",
        "        to the dimensions of the observation distributions of the underlying\n",
        "        Markov chain.  The next dimension from the right indexes the steps in a\n",
        "        sequence of observations from a single sample from the hidden Markov\n",
        "        model.  The size of this dimension should match the `num_steps`\n",
        "        parameter of the hidden Markov model object.  The other dimensions are\n",
        "        the dimensions of the batch and these are broadcast with the hidden\n",
        "        Markov model's parameters.\n",
        "      mask: optional bool-type `tensor` with rightmost dimension matching\n",
        "        `num_steps` indicating which observations the result of this\n",
        "        function should be conditioned on. When the mask has value\n",
        "        `True` the corresponding observations aren't used.\n",
        "        if `mask` is `None` then all of the observations are used.\n",
        "        the `mask` dimensions left of the last are broadcast with the\n",
        "        hmm batch as well as with the observations.\n",
        "      name: Python `str` name prefixed to Ops created by this class.\n",
        "        Default value: \"HiddenMarkovModel\".\n",
        "    Returns:\n",
        "      posterior_mode: A `Tensor` representing the most likely sequence of hidden\n",
        "        states. The rightmost dimension of this tensor will equal the\n",
        "        `num_steps` parameter providing one hidden state for each step. The\n",
        "        other dimensions are those of the batch.\n",
        "    Raises:\n",
        "      ValueError: if the `observations` tensor does not consist of\n",
        "      sequences of `num_steps` observations.\n",
        "    #### Examples\n",
        "    ```python\n",
        "    tfd = tfp.distributions\n",
        "    # A simple weather model.\n",
        "    # Represent a cold day with 0 and a hot day with 1.\n",
        "    # Suppose the first day of a sequence has a 0.8 chance of being cold.\n",
        "    initial_distribution = tfd.Categorical(probs=[0.8, 0.2])\n",
        "    # Suppose a cold day has a 30% chance of being followed by a hot day\n",
        "    # and a hot day has a 20% chance of being followed by a cold day.\n",
        "    transition_distribution = tfd.Categorical(probs=[[0.7, 0.3],\n",
        "                                                     [0.2, 0.8]])\n",
        "    # Suppose additionally that on each day the temperature is\n",
        "    # normally distributed with mean and standard deviation 0 and 5 on\n",
        "    # a cold day and mean and standard deviation 15 and 10 on a hot day.\n",
        "    observation_distribution = tfd.Normal(loc=[0., 15.], scale=[5., 10.])\n",
        "    # This gives the hidden Markov model:\n",
        "    model = tfd.HiddenMarkovModel(\n",
        "        initial_distribution=initial_distribution,\n",
        "        transition_distribution=transition_distribution,\n",
        "        observation_distribution=observation_distribution,\n",
        "        num_steps=7)\n",
        "    # Suppose we observe gradually rising temperatures over a week:\n",
        "    temps = [-2., 0., 2., 4., 6., 8., 10.]\n",
        "    # We can now compute the most probable sequence of hidden states:\n",
        "    model.posterior_mode(temps)\n",
        "    # The result is [0 0 0 0 0 1 1] telling us that the transition\n",
        "    # from \"cold\" to \"hot\" most likely happened between the\n",
        "    # 5th and 6th days.\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    with self._name_and_control_scope(name):\n",
        "      observations = tf.convert_to_tensor(observations, name='observations')\n",
        "      if mask is not None:\n",
        "        mask = tf.convert_to_tensor(mask, name='mask', dtype_hint=tf.bool)\n",
        "      num_states = self.transition_distribution.batch_shape_tensor()[-1]\n",
        "      observation_distribution = self.observation_distribution\n",
        "      underlying_event_rank = tf.size(\n",
        "          observation_distribution.event_shape_tensor())\n",
        "      observation_tensor_shape = tf.shape(observations)\n",
        "      mask_tensor_shape = tf.shape(mask) if mask is not None else None\n",
        "\n",
        "      with self._observation_mask_shape_preconditions(\n",
        "          observation_tensor_shape, mask_tensor_shape, underlying_event_rank):\n",
        "        observation_log_probs = self._observation_log_probs(\n",
        "            observations, mask)\n",
        "        log_init = _extract_log_probs(num_states,\n",
        "                                      self.initial_distribution)\n",
        "        log_trans = _extract_log_probs(num_states,\n",
        "                                       self.transition_distribution)\n",
        "        log_prob = log_init + observation_log_probs[0]\n",
        "\n",
        "        def _reduce_multiple_steps():\n",
        "          \"\"\"Perform `reduce_max` operation when `num_steps` > 1.\"\"\"\n",
        "\n",
        "          def forward_step(previous_step_pair, log_prob_observation):\n",
        "            log_prob_previous = previous_step_pair[0]\n",
        "            log_prob = (log_prob_previous[..., tf.newaxis] +\n",
        "                        log_trans +\n",
        "                        log_prob_observation[..., tf.newaxis, :])\n",
        "            most_likely_given_successor = tf.argmax(log_prob, axis=-2)\n",
        "            max_log_p_given_successor = tf.reduce_max(log_prob,\n",
        "                                                      axis=-2)\n",
        "            return (max_log_p_given_successor, most_likely_given_successor)\n",
        "\n",
        "          forward_log_probs, all_most_likely_given_successor = tf.scan(\n",
        "              forward_step,\n",
        "              observation_log_probs[1:],\n",
        "              initializer=(log_prob,\n",
        "                           tf.zeros(tf.shape(log_prob),\n",
        "                                    dtype=tf.int64)),\n",
        "              name='forward_log_probs')\n",
        "\n",
        "          most_likely_end = tf.argmax(forward_log_probs[-1], axis=-1)\n",
        "\n",
        "          # We require the operation that gives C from A and B where\n",
        "          # C[i...j] = A[i...j, B[i...j]]\n",
        "          # and A = most_likely_given_successor\n",
        "          #     B = most_likely_successor.\n",
        "          # tf.gather requires indices of known shape so instead we use\n",
        "          # reduction with tf.one_hot(B) to pick out elements from B\n",
        "          def backward_step(most_likely_successor,\n",
        "                            most_likely_given_successor):\n",
        "            return tf.reduce_sum(\n",
        "                (most_likely_given_successor *\n",
        "                 tf.one_hot(most_likely_successor,\n",
        "                            num_states,\n",
        "                            dtype=tf.int64)),\n",
        "                axis=-1)\n",
        "\n",
        "          backward_scan = tf.scan(\n",
        "              backward_step,\n",
        "              all_most_likely_given_successor,\n",
        "              most_likely_end,\n",
        "              reverse=True)\n",
        "          most_likely_sequences = tf.concat([backward_scan,\n",
        "                                             [most_likely_end]],\n",
        "                                            axis=0)\n",
        "          return distribution_util.move_dimension(\n",
        "              most_likely_sequences, 0, -1)\n",
        "\n",
        "        return prefer_static.cond(\n",
        "            self.num_steps > 1,\n",
        "            _reduce_multiple_steps,\n",
        "            lambda: tf.argmax(log_prob, axis=-1)[..., tf.newaxis])\n",
        "\n",
        "  # pylint: disable=protected-access\n",
        "  def _default_event_space_bijector(self):\n",
        "    return (self._observation_distribution.\n",
        "            _experimental_default_event_space_bijector())\n",
        "  # pylint: enable=protected-access\n",
        "\n",
        "  def _parameter_control_dependencies(self, is_init):\n",
        "    assertions = []\n",
        "\n",
        "    # Check num_steps is a scalar that's at least 1.\n",
        "    if is_init != tensor_util.is_ref(self.num_steps):\n",
        "      num_steps = tf.convert_to_tensor(self.num_steps)\n",
        "      num_steps_ = tf.get_static_value(num_steps)\n",
        "      if num_steps_ is not None:\n",
        "        if np.ndim(num_steps_) != 0:\n",
        "          raise ValueError(\n",
        "              '`num_steps` must be a scalar but it has rank {}'.format(\n",
        "                  np.ndim(num_steps_)))\n",
        "        if num_steps_ < 1:\n",
        "          raise ValueError('`num_steps` must be at least 1.')\n",
        "      elif self.validate_args:\n",
        "        message = '`num_steps` must be a scalar'\n",
        "        assertions.append(\n",
        "            assert_util.assert_rank_at_most(self.num_steps, 0, message=message))\n",
        "        assertions.append(\n",
        "            assert_util.assert_greater_equal(\n",
        "                num_steps, 1,\n",
        "                message='`num_steps` must be at least 1.'))\n",
        "\n",
        "    # Check that the initial distribution has scalar events over the\n",
        "    # integers.\n",
        "    if is_init and not dtype_util.is_integer(self.initial_distribution.dtype):\n",
        "      raise ValueError(\n",
        "          '`initial_distribution.dtype` ({}) is not over integers'.format(\n",
        "              dtype_util.name(self.initial_distribution.dtype)))\n",
        "\n",
        "    if tensorshape_util.rank(self.initial_distribution.event_shape) is not None:\n",
        "      if tensorshape_util.rank(self.initial_distribution.event_shape) != 0:\n",
        "        raise ValueError('`initial_distribution` must have scalar `event_dim`s')\n",
        "    elif self.validate_args:\n",
        "      assertions += [\n",
        "          assert_util.assert_equal(\n",
        "              tf.size(self.initial_distribution.event_shape_tensor()),\n",
        "              0,\n",
        "              message='`initial_distribution` must have scalar `event_dim`s'),\n",
        "      ]\n",
        "\n",
        "    # Check that the transition distribution is over the integers.\n",
        "    if (is_init and\n",
        "        not dtype_util.is_integer(self.transition_distribution.dtype)):\n",
        "      raise ValueError(\n",
        "          '`transition_distribution.dtype` ({}) is not over integers'.format(\n",
        "              dtype_util.name(self.transition_distribution.dtype)))\n",
        "\n",
        "    # Check observations have non-scalar batches.\n",
        "    # The graph version of this assertion is incorporated as\n",
        "    # a control dependency of the transition/observation\n",
        "    # compatibility test.\n",
        "    if tensorshape_util.rank(self.observation_distribution.batch_shape) == 0:\n",
        "      raise ValueError(\n",
        "          \"`observation_distribution` can't have scalar batches\")\n",
        "\n",
        "    # Check transitions have non-scalar batches.\n",
        "    # The graph version of this assertion is incorporated as\n",
        "    # a control dependency of the transition/observation\n",
        "    # compatibility test.\n",
        "    if tensorshape_util.rank(self.transition_distribution.batch_shape) == 0:\n",
        "      raise ValueError(\n",
        "          \"`transition_distribution` can't have scalar batches\")\n",
        "\n",
        "    # Check compatibility of transition distribution and observation\n",
        "    # distribution.\n",
        "    tdbs = self.transition_distribution.batch_shape\n",
        "    odbs = self.observation_distribution.batch_shape\n",
        "    if (tensorshape_util.dims(tdbs) is not None and\n",
        "        tf.compat.dimension_value(odbs[-1]) is not None):\n",
        "      if (tf.compat.dimension_value(tdbs[-1]) !=\n",
        "          tf.compat.dimension_value(odbs[-1])):\n",
        "        raise ValueError(\n",
        "            '`transition_distribution` and `observation_distribution` '\n",
        "            'must agree on last dimension of batch size')\n",
        "    elif self.validate_args:\n",
        "      tdbs = self.transition_distribution.batch_shape_tensor()\n",
        "      odbs = self.observation_distribution.batch_shape_tensor()\n",
        "      transition_precondition = assert_util.assert_greater(\n",
        "          tf.size(tdbs), 0,\n",
        "          message=('`transition_distribution` can\\'t have scalar '\n",
        "                   'batches'))\n",
        "      observation_precondition = assert_util.assert_greater(\n",
        "          tf.size(odbs), 0,\n",
        "          message=('`observation_distribution` can\\'t have scalar '\n",
        "                   'batches'))\n",
        "      with tf.control_dependencies([\n",
        "          transition_precondition,\n",
        "          observation_precondition]):\n",
        "        assertions += [\n",
        "            assert_util.assert_equal(\n",
        "                tdbs[-1],\n",
        "                odbs[-1],\n",
        "                message=('`transition_distribution` and '\n",
        "                         '`observation_distribution` '\n",
        "                         'must agree on last dimension of batch size'))]\n",
        "\n",
        "    return assertions\n",
        "\n",
        "\n",
        "def _log_vector_matrix(vs, ms):\n",
        "  \"\"\"Multiply tensor of vectors by matrices assuming values stored are logs.\"\"\"\n",
        "\n",
        "  return tf.reduce_logsumexp(vs[..., tf.newaxis] + ms, axis=-2)\n",
        "\n",
        "\n",
        "def _log_matrix_vector(ms, vs):\n",
        "  \"\"\"Multiply tensor of matrices by vectors assuming values stored are logs.\"\"\"\n",
        "\n",
        "  return tf.reduce_logsumexp(ms + vs[..., tf.newaxis, :], axis=-1)\n",
        "\n",
        "\n",
        "def _vector_matrix(vs, ms):\n",
        "  \"\"\"Multiply tensor of vectors by matrices.\"\"\"\n",
        "\n",
        "  return tf.reduce_sum(vs[..., tf.newaxis] * ms, axis=-2)\n",
        "\n",
        "\n",
        "def _extract_log_probs(num_states, dist):\n",
        "  \"\"\"Tabulate log probabilities from a batch of distributions.\"\"\"\n",
        "\n",
        "  states = tf.reshape(tf.range(num_states),\n",
        "                      tf.concat([[num_states],\n",
        "                                 tf.ones_like(dist.batch_shape_tensor())],\n",
        "                                axis=0))\n",
        "  return distribution_util.move_dimension(dist.log_prob(states), 0, -1)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}